{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Challenge: Partitioning Two-Thousand Stocks by their return series\n",
    "Attached .csv file contains return series of 2,000 stocks(columns) for 3,000 days(rows). Please make 10 partition groups of 2,000 stocks satisfying following conditions.\n",
    "\n",
    "1. Each partition group has 200 stocks.\n",
    "\n",
    "2. Minimize inter-group return correlation and maximize intra-group return correlation.\n",
    "Objective function will be CARS/TICS.\n",
    "\n",
    "3. Intra-group return correlation TICS is defined as below.\n",
    "For each partition group with label k, 200x200 correlation matrix IC(k) can be calculated.\n",
    "ICS(k) is a summation of all elements in IC(k).\n",
    "TICS = ICS(1) + ICS(2) + … + ICS(10)\n",
    "\n",
    "4. Inter-group return correlation CARS is defines as below.\n",
    "AR(k) is a return series obtained by averaging over 200 stocks’ return series. Then AR(k) is\n",
    "a vector with 3,000 elements(3,000 days). CAR is 10x10 correlation matrix of [AR(1), AR(2),\n",
    "… AR(10)] and CARS is a summation of all elements in CAR matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition:\n",
    "For normal Machine Learning clustering algorithms, such as K-Means algorithm, the absolute co-ordinates of points are needed.\n",
    "The correlation problem can be seen as a clustering problem in which only relative distances of points are provided.Without absolute co-ordinates, normal clustering algorithms are not applicable in solving this problem. \n",
    "Also, directly importing existing Machine Learning Python packages is not applicable, and we need to re-write a class based on our unique requirements.\n",
    "\n",
    "Hence, I re-wrote Hierarchical Clustering algorithm to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1: No requirement of same sizes of clusters.\n",
    "### 1.1.1 Description\n",
    "\n",
    "You are required to use Python 2.7 or C++ as your programming language. You mayor may not automate all the process which the question is asking you to solve. If you think a certain part takes longer time foryou to code, you may skipthat part and manually compete the task. Task is mainly on data science problem.\n",
    "\n",
    "Please download daily price data of about 500 stocks in S&P 500 to calculate daily return series of 500 stock in S&P500. Now you can calculate correlation between 500 stocksfrom return series. This will be written as 500 x500 matrix with ones along the diagonal. \n",
    "\n",
    "Let's define Corr(A,B) as the correlation coefficient between A and B derived from their return series. Then, build clusters satisfying the following conditions:\n",
    "Correlation between any pair of stock in the same cluster must be higher than pair of stocks from different cluster. \n",
    "For example, A and B are from one cluster and C are foranother cluster;\n",
    "Corr(A,B) >= Corr(A,C) and Corr(A,B) >= Corr(B,C)\n",
    "\n",
    "Then, you are now supposed to find clusters defined above to optimize the following objective functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Requirements:\n",
    "\n",
    "a. Summation over Correlations between stocks in the same cluster is maximized.This sum of correlation is sum of all the elements in correlation matrix.I.e.ifyou constructed aclusterwith100 stocks.You can calculate 100 x 100 correlation matrix and sum up all the elements to get \"sum of correlation\". In other word, stocksin the same cluster must be highly correlated. \n",
    "\n",
    "b. The number of clusters is not restricted as long as you have more than four clusters. But standard deviation of the numbers of clusters is to be minimized. For example, clusters of 100 stocks, 100 stocks, 100 stocks and 200 stocks are better than those of 300 stocks, 160 stocks, 35 stocks and 5 stocks. \n",
    "\n",
    "c. You can calculate the average of returns of stocks in the same cluster. If you have four clusters, you will have four average returns, from which 4x4 correlation matrix can be calculated. Summation over Correlation between average returns of clusters is minimized. In other word, clusters must not be correlated in termsof average return.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data, Code and Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "503 stocksfrom S&P 500 have available daily prices fromYahoo! Finance and Quandl databases. \n",
    "\n",
    "Intra-group return correlation TICS is defined as below. \n",
    "For each partition group with label k, 200x200 correlation matrix IC(k) can be calculated. ICS(k) is a summation of all elements in IC(k). TICS = ICS(1) + ICS(2) + … + ICS(10).\n",
    "\n",
    "Inter-group return correlation CARS is defines as below. \n",
    "AR(k)is a return series obtained by averaging over 200 stocks’return series.ThenAR(k)is a vector with 3,000 elements(3,000 days). CAR is 10x10 correlation matrix of [AR(1),AR(2), …AR(10)] and CARS is a summation of all elements in CAR matrix.\n",
    "\n",
    "To find clusters with maximum TICS, maximum CARS and similar sizes, the results are as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Code for downloading stock returns from Yahoo! Finance, Quandl and IEX datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fix_yahoo_finance as yf\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "from datetime import datetime,date\n",
    "import numpy as np\n",
    "#Download daily prices from Yahoo! Finance\n",
    "def yahooFinanceDownload(ticker,filepath,start_date,end_date):\n",
    "    prices = yf.download(ticker, start=start_date, end=end_date) \n",
    "    if len(prices):\n",
    "        out_filename = filepath + ticker + '.csv'\n",
    "        prices.to_csv(out_filename)\n",
    "\n",
    "def quandlDownload(ticker,filepath):\n",
    "    start = datetime(2000,1,1)\n",
    "    end = date.today()\n",
    "    try:\n",
    "        prices = web.DataReader(ticker, 'quandl', start, end)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        out_filename = filepath + ticker + '.csv'\n",
    "        prices.to_csv(out_filename)\n",
    "    \n",
    "def collectStockName(filepath):\n",
    "    stock_list=[]\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        if files:\n",
    "            for f in files:\n",
    "                if 'csv' in f:\n",
    "                    stock_list.append(f.split('.csv')[0])\n",
    "    return stock_list\n",
    "\n",
    "def getDownloadList(ticker_list,filepath):\n",
    "    download_list = collectStockName(filepath)\n",
    "    unsuccess_list = [ticker for ticker in ticker_list if ticker not in download_list]\n",
    "    return download_list, unsuccess_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self,index_list,position=None,count=1):\n",
    "        self.count = count\n",
    "        self.index_list = index_list  \n",
    "        self.position = position\n",
    "\n",
    "class EqualSizedClustering(object):\n",
    "    def __init__(self,corr_matrix,k,node_size=None):\n",
    "        self.k = k\n",
    "        self.corr_matrix = corr_matrix\n",
    "        self._len = np.shape(self.corr_matrix)[0]\n",
    "        if node_size is None:\n",
    "            self.node_size = int(self._len/self.k)\n",
    "        else:\n",
    "            self.node_size = node_size\n",
    "        self.nodes = []\n",
    "\n",
    "        self.all_index = [i for i in range(self._len)]\n",
    "        self.df = None\n",
    "\n",
    "    def initial_fit(self):\n",
    "        nodes = []\n",
    "        for i in self.all_index:\n",
    "            nodes.append(Node(index_list=[i],position=i,count=1))\n",
    "        position = -1\n",
    "        distances = {}\n",
    "\n",
    "        new_nodes = []\n",
    "\n",
    "        while True:\n",
    "            max_dist = 0\n",
    "            nodes_len = len(nodes)\n",
    "\n",
    "            for i in range(nodes_len-1):\n",
    "                for j in range(i+1,nodes_len):\n",
    "                    node_a,node_b = nodes[i],nodes[j]\n",
    "                    position_pair = (node_a.position,node_b.position)\n",
    "                    if position_pair not in distances:\n",
    "                        distances[position_pair] = self.node_distance(node_a,node_b)\n",
    "                    d = distances[position_pair]\n",
    "                    if max_dist < d:\n",
    "                        max_dist = d\n",
    "                        max_i = i\n",
    "                        max_j = j\n",
    "                        max_node_a = node_a\n",
    "                        max_node_b = node_b\n",
    "                        \n",
    "            node_a = max_node_a\n",
    "            node_b = max_node_b\n",
    "\n",
    "            new_index_list = node_a.index_list + node_b.index_list\n",
    "            new_count = node_a.count + node_b.count\n",
    "            new_node = Node(index_list=new_index_list,count=new_count,position=position) \n",
    "            if len(nodes) > 2:\n",
    "                nodes.pop(max_j)\n",
    "                nodes.pop(max_i)\n",
    "            else:\n",
    "                node_a = nodes[0]\n",
    "                node_b = nodes[1]\n",
    "                new_index_list = node_a.index_list + node_b.index_list\n",
    "                new_count = node_a.count + node_b.count\n",
    "                new_node = Node(index_list=new_index_list,count=new_count,position=position) \n",
    "                new_nodes.append(new_node)\n",
    "                break\n",
    "            \n",
    "            if new_node.count >= self.node_size:\n",
    "                new_nodes.append(new_node)\n",
    "            else:             \n",
    "                nodes.append(new_node)\n",
    "            position -= 1\n",
    "\n",
    "            if len(nodes)+len(new_nodes) == self.k :\n",
    "                if len(nodes) > 0:\n",
    "                    new_nodes.extend(nodes)\n",
    "                break\n",
    "\n",
    "            \n",
    "        self.nodes = new_nodes\n",
    "\n",
    "    def adjust_size(self):\n",
    "        exact_size = [node for node in self.nodes if node.count == self.node_size]\n",
    "        large_size = [node for node in self.nodes if node.count > self.node_size]\n",
    "        small_size = [node for node in self.nodes if node.count < self.node_size]\n",
    "        from_large_to_small = []\n",
    "        if large_size != []:\n",
    "            for node in large_size:\n",
    "                n = node.count - self.node_size\n",
    "                remove_index = self.remove_smallest_index(node,n,\n",
    "                                                          distance=False,all_index=self.all_index)\n",
    "                #remove_index = self.remove_smallest_index(node,n)\n",
    "                from_large_to_small.extend(remove_index)\n",
    "                for index in remove_index:\n",
    "                    node.index_list.remove(index)\n",
    "                exact_size.append(node)             \n",
    "            for node in small_size:\n",
    "                n = self.node_size - node.count\n",
    "                temp_all_index = [index for index in self.all_index if index not in from_large_to_small]\n",
    "                add_index = self.add_largest_index(node,from_large_to_small,n,distance=False,all_index=temp_all_index)\n",
    "                \n",
    "                #add_index = self.add_largest_index(node,from_large_to_small,n)\n",
    "                node.index_list.extend(add_index)\n",
    "                exact_size.append(node)\n",
    "                for index in add_index:\n",
    "                    from_large_to_small.remove(index)\n",
    "        self.nodes = exact_size\n",
    "\n",
    "        \n",
    "    def setDataDf(self,df):\n",
    "        self.df = df   \n",
    "        \n",
    "    def ICS(self,node):\n",
    "        node_corr_matrix = self.corr_matrix[node.index_list][:,node.index_list]\n",
    "        return node_corr_matrix.sum()\n",
    "\n",
    "    def TICS(self):\n",
    "        TICS = 0\n",
    "        for node in self.nodes:\n",
    "            TICS += self.ICS(node)\n",
    "        return TICS\n",
    "\n",
    "    def AR(self,node):\n",
    "        temp = self.df[node.index_list]\n",
    "        return temp.mean(axis=1)\n",
    "    \n",
    "    def CARS(self):\n",
    "        output = pd.DataFrame(index=self.df.index)\n",
    "        id = 0\n",
    "        for node in self.nodes:\n",
    "            id += 1\n",
    "            output[id] = self.AR(node)\n",
    "        temp = np.array(output.corr())\n",
    "        cars = temp.sum()\n",
    "        return cars\n",
    "        \n",
    "    def node_distance(self,node_a,node_b):\n",
    "        temp = self.corr_matrix[node_b.index_list][:,node_a.index_list]        \n",
    "        return temp.mean()\n",
    "\n",
    "\n",
    "    def output_results(self,output_filename):\n",
    "        label = 0\n",
    "        index_list,label_list = [],[]\n",
    "        for node in self.nodes:\n",
    "            label += 1\n",
    "            for index in node.index_list:\n",
    "                index_list.append(index)\n",
    "                label_list.append(label)\n",
    "        output = pd.DataFrame({'Stock':pd.Series(index_list),'Label':pd.Series(label_list)})\n",
    "        output.to_csv(output_filename)\n",
    "        \n",
    "    def current_target_value(self):\n",
    "        cars = self.CARS()\n",
    "        tics = self.TICS()\n",
    "        value = cars/tics\n",
    "        return cars,tics,value\n",
    "    \n",
    "    def remove_smallest_index(self,node,remove_num,distance=True,all_index=None):\n",
    "        index_list = node.index_list\n",
    "        remove_index = []\n",
    "        contributions = np.zeros(len(index_list))\n",
    "        for pos in range(len(index_list)):\n",
    "            index = index_list[pos]\n",
    "            temp_node = Node(index_list=[index])\n",
    "            if distance:\n",
    "                contributions[pos]= self.node_distance(node,temp_node)\n",
    "            else:\n",
    "                contributions[pos] = self.relative_contributions(node,index,all_index)\n",
    "        for select_time in range(remove_num):\n",
    "            pos = np.where(contributions==contributions.min())[0][0]\n",
    "            remove_index.append(index_list[pos])\n",
    "            contributions[pos] = 1000\n",
    "        return remove_index\n",
    "    \n",
    "    def deleteIndex(self,del_index,index_list):\n",
    "        new_list = []\n",
    "\n",
    "        for index in index_list:\n",
    "            if index != del_index:\n",
    "                new_list.append(index)\n",
    "        return new_list\n",
    "    \n",
    "    def relative_contributions(self,node,index,all_index):\n",
    "        index_list = self.deleteIndex(index,node.index_list)\n",
    "        return self.corr_matrix[[index]][:,index_list].sum()/self.corr_matrix[[index]][:,all_index].sum()\n",
    "        \n",
    "    def add_largest_index(self,node,add_index_candidate,add_num,distance=True,all_index=None):\n",
    "        add_index = []\n",
    "        contributions = np.zeros(len(add_index_candidate)) \n",
    "        for pos in range(len(add_index_candidate)):\n",
    "            index = add_index_candidate[pos]\n",
    "            temp_node = Node(index_list=[index])\n",
    "            if distance:\n",
    "                contributions[pos]= self.node_distance(node,temp_node)\n",
    "            else:\n",
    "                contributions[pos] = self.relative_contributions(node,index,all_index)\n",
    "        for select_time in range(add_num):\n",
    "            pos = np.where(contributions==contributions.max())[0][0]\n",
    "            add_index.append(add_index_candidate[pos])\n",
    "            contributions[pos] = -1000\n",
    "        return add_index\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To find clusters with maximum TICS, maximum CARS and similar sizes, the results are as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_c201():\n",
    "    corr = pd.read_csv('D:/c102/c102_corr.csv')\n",
    "    corr_matrix = np.array(corr)\n",
    "    eqc = EqualSizedClustering(corr_matrix=corr_matrix,k=4,node_size=125)\n",
    "    eqc.initial_fit()\n",
    "    \n",
    "    df = pd.read_csv('D:/c102/c102_data.csv')\n",
    "    df = df.set_index('Date')\n",
    "\n",
    "    df.columns = [i for i in range(503)]\n",
    "    \n",
    "    eqc.setDataDf(df)\n",
    "    current_value = eqc.current_target_value()\n",
    "    output = pd.DataFrame()\n",
    "    output.loc['Result', 'CARS'] = current_value[0]\n",
    "    output.loc['Result', 'TICS'] = current_value[1]\n",
    "    output.loc['Result', 'Target Value'] = current_value[2]\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             CARS          TICS  Target Value\n",
      "Result  14.455305  32423.822044      0.000446\n"
     ]
    }
   ],
   "source": [
    "if __name__== '__main__':\n",
    "    test_c201()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2: Clusters must be with equal sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attached .csv file contains return series of 2,000 stocks(columns) for 3,000 days(rows). Please make\n",
    "10 partition groups of 2,000 stocks satisfying following conditions.\n",
    "1. Each partition group has 200 stocks.\n",
    "2. Minimize inter-group return correlation and maximize intra-group return correlation.\n",
    "Objective function will be CARS/TICS.\n",
    "3. Intra-group return correlation TICS is defined as below.\n",
    "For each partition group with label k, 200x200 correlation matrix IC(k) can be calculated.\n",
    "ICS(k) is a summation of all elements in IC(k).\n",
    "TICS = ICS(1) + ICS(2) + … + ICS(10)\n",
    "4. Inter-group return correlation CARS is defines as below.\n",
    "AR(k) is a return series obtained by averaging over 200 stocks’ return series. Then AR(k) is\n",
    "a vector with 3,000 elements(3,000 days). CAR is 10x10 correlation matrix of [AR(1), AR(2),\n",
    "… AR(10)] and CARS is a summation of all elements in CAR matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_HFC():\n",
    "    df = pd.read_csv('D:/54_hfc_20170614_comp.csv')\n",
    "    df.columns = [i for i in range(2000)]\n",
    "    corr = df.corr()\n",
    "    corr_matrix = np.array(corr)\n",
    "    eqc = EqualSizedClustering(corr_matrix=corr_matrix,k=10)\n",
    "    eqc.initial_fit()   \n",
    "    eqc.setDataDf(df)\n",
    "    current_value = eqc.current_target_value()\n",
    "    output = pd.DataFrame()\n",
    "    output.loc['Not Equal Sized Clusters', 'CARS'] = current_value[0]\n",
    "    output.loc['Not Equal Sized Clusters', 'TICS'] = current_value[1]\n",
    "    output.loc['Not Equal Sized Clusters', 'Target Value'] = current_value[2]\n",
    "    eqc.adjust_size()\n",
    "    value = eqc.current_target_value()\n",
    "    output.loc['Equal Sized Clusters', 'CARS'] =value[0]\n",
    "    output.loc['Equal Sized Clusters', 'TICS'] =value[1]\n",
    "    output.loc['Equal Sized Clusters', 'Target Value'] = value[2]\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               CARS           TICS  Target Value\n",
      "Not Equal Sized Clusters  67.048950  151963.456286      0.000441\n",
      "Equal Sized Clusters      80.936464  135266.081639      0.000598\n"
     ]
    }
   ],
   "source": [
    "if __name__== '__main__':\n",
    "    # This takes arount 40 minutes to get the results. Please be patient.\\ \n",
    "    test_HFC()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
